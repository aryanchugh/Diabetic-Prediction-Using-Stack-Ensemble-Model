# -*- coding: utf-8 -*-
"""Diabetic Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KTy61_gEuCjjo9hPr32VK6B-JfJEdGfM

# **Installing & Importing Libraries**
"""

!pip install vecstack

# Commented out IPython magic to ensure Python compatibility.
# data wrangling & pre-processing
#import pandas as pd
#import numpy as np

# data visualization
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn

#to split Dataset
from sklearn.model_selection import train_test_split

#model validation
from sklearn.metrics import log_loss,roc_auc_score,precision_score,f1_score,recall_score,roc_curve,auc
from sklearn.metrics import classification_report, confusion_matrix,accuracy_score,fbeta_score,matthews_corrcoef
from sklearn import metrics

# cross validation
from sklearn.model_selection import StratifiedKFold

# machine learning algorithms
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier,VotingClassifier,AdaBoostClassifier,GradientBoostingClassifier,RandomForestClassifier,ExtraTreesClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import SGDClassifier
from sklearn.svm import SVC
import xgboost as xgb

import lightgbm
from lightgbm import LGBMClassifier

from vecstack import stacking

#R
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB

#Ensemble
from imblearn.metrics import geometric_mean_score
import re
import sklearn
from sklearn.model_selection import StratifiedKFold
import os
from sklearn.calibration import CalibratedClassifierCV

from scipy import stats

import warnings
warnings.filterwarnings('ignore')

"""# **Loading Data**"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

df = pd.read_csv('/content/drive/MyDrive/Dataset Diabetes 2019/diabetes_dataset__2019.csv')
df

df.describe()

df.info()

df.isna().sum()

"""# **Profile report**"""

#!pip install --upgrade pandas_profiling

#from pandas_profiling import ProfileReport
#df.profile_report()

"""# **Data Analysis**"""

df[['BMI', 'Sleep','SoundSleep']].hist(figsize=(20,10))
plt.show()

import seaborn
print(df['Diabetic'].value_counts())
df['Diabetic'].replace(' no', 'no', inplace=True)
print(df['Diabetic'].value_counts())
seaborn.countplot(x = 'Diabetic', data = df)

seaborn.pairplot(data=df, hue='Diabetic')

fig = plt.figure(figsize= (12,5))
plt.subplot(131)
seaborn.violinplot(data = df, x = 'Diabetic', y = 'BMI')
plt.subplot(132)
seaborn.violinplot(data = df, x = 'Diabetic', y = 'Sleep')
plt.subplot(133)
seaborn.violinplot(data = df, x = 'Diabetic', y = 'SoundSleep')

cols = ['Age', 'Gender', 'Family_Diabetes', 'highBP', 'PhysicallyActive',
       'Smoking', 'Alcohol', 'RegularMedicine',
       'JunkFood', 'Stress', 'BPLevel', 'Pregancies', 'Pdiabetes',
       'UriationFreq']

plt.figure(figsize = (15,25))

i  = 0
for j in range(9):
    plt.xticks(rotation=75)
    plt.subplot(int(str(3)+str(3)+str(j+1)))
    seaborn.countplot(x = cols[i], hue='Diabetic',data = df)
    i += 1
plt.show()

plt.figure(figsize = (15,25))
for j in range(5):
    plt.xticks(rotation=75)
    plt.subplot(int(str(3)+str(3)+str(j+1)))
    seaborn.countplot(x = cols[i], hue='Diabetic',data = df)
    i += 1

"""# **Pre-processing the data**"""

standard_df = df.copy()
# y = standard_df['Diabetic_yes']
# x = standard_df.drop(columns = ['Diabetic_yes'])

from sklearn.preprocessing import StandardScaler
cols = ['BMI','Sleep','SoundSleep']
standard_df[cols] = StandardScaler().fit_transform(standard_df[cols].values)
standard_df

df.columns

df

df.describe()

indexes = df[df['Diabetic'].isna() | df['Pdiabetes'].isna() | df['BMI'].isna()].index.to_list()

print(indexes)
df.drop(index= indexes,inplace = True)

print(df['Pregancies'].value_counts())
df['Pregancies'].fillna(value = 0.0, inplace= True)
print(df['Pregancies'].value_counts())
df['Pregancies'] = df['Pregancies'].astype(int)
df['Pregancies']

print(df['RegularMedicine'].value_counts())
df['RegularMedicine'].replace('o', 'no', inplace =True)
print(df['RegularMedicine'].value_counts())

print(df['Pdiabetes'].value_counts())
df['Pdiabetes'].replace('0', 'no', inplace = True)
print(df['Pdiabetes'].value_counts())

df = pd.get_dummies(df, drop_first= True)

preg = pd.get_dummies(df['Pregancies'],prefix='Pregnancies',drop_first= True)

print(preg.head())

df = pd.concat([preg,df], axis = 1)
df.drop(columns=['Pregancies'],inplace=True)

"""# **Train Test Split & Feature normalization & building baseline model**"""

y = df['Diabetic_yes']
X = df.drop(columns= ['Diabetic_yes'])

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2,shuffle=True, random_state=5)

## checking distribution of traget variable in train test split
print('Distribution of target variable in training set')
print(y_train.value_counts())

print('Distribution of target variable in test set')
print(y_test.value_counts())

print('Training Set\n')
print(X_train.shape)
print(y_train.shape)

print('\nTest Set\n')
print(X_test.shape)
print(y_test.shape)

print(X.shape)
print(y.shape)

if X.shape[0] != y.shape[0]:
  print("X and y rows are mismatched, check dataset again")

from sklearn import model_selection
from sklearn.model_selection import cross_val_score

# function initializing baseline machine learning models
def GetBasedModel():
    basedModels = []
    basedModels.append(('LR_L2'   , LogisticRegression(penalty='l2')))
    basedModels.append(('LDA'  , LinearDiscriminantAnalysis()))
    basedModels.append(('KNN7'  , KNeighborsClassifier(7)))
    basedModels.append(('KNN5'  , KNeighborsClassifier(5)))
    basedModels.append(('KNN9'  , KNeighborsClassifier(9)))
    basedModels.append(('KNN11'  , KNeighborsClassifier(11)))
    basedModels.append(('NB'   , GaussianNB()))
    basedModels.append(('SVM Linear'  , SVC(kernel='linear',gamma='auto',probability=True)))
    basedModels.append(('SVM RBF'  , SVC(kernel='rbf',gamma='auto',probability=True)))
    basedModels.append(('AB'   , AdaBoostClassifier()))

    basedModels.append(('MLP', MLPClassifier()))
    basedModels.append(('SGD3000', SGDClassifier(max_iter=1000, tol=1e-4)))
    basedModels.append(('XGB_2000', xgb.XGBClassifier(n_estimators= 2000)))
    basedModels.append(('XGB_500', xgb.XGBClassifier(n_estimators= 500)))
    basedModels.append(('XGB_100', xgb.XGBClassifier(n_estimators= 100)))
    basedModels.append(('XGB_1000', xgb.XGBClassifier(n_estimators= 1000)))
    basedModels.append(('DTC' , DecisionTreeClassifier()))
    basedModels.append(('GBM'  , GradientBoostingClassifier(n_estimators=100,max_features='sqrt')))
    basedModels.append(('RF_Ent100'   , RandomForestClassifier(criterion='entropy',n_estimators=100)))
    basedModels.append(('RF_Gini100'   , RandomForestClassifier(criterion='gini',n_estimators=100)))
    basedModels.append(('ET100'   , ExtraTreesClassifier(n_estimators= 100)))
    basedModels.append(('ET500'   , ExtraTreesClassifier(n_estimators= 500)))
    basedModels.append(('ET1000'   , ExtraTreesClassifier(n_estimators= 1000)))

    basedModels.append(('LGBMC', LGBMClassifier()))


    return basedModels

# function for performing 10-fold cross validation of all the baseline models
def BasedLine2(X_train, y_train,models):
    # Test options and evaluation metric
    num_folds = 10
    scoring = 'accuracy'
    seed = 7
    results = []
    names = []
    for name, model in models:
        kfold = model_selection.KFold(n_splits=10, shuffle=True, random_state=seed)
        cv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)
        results.append(cv_results)
        names.append(name)
        msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
        print(msg)


    return results,msg

"""# **Ensembling with Stacked Classifier**

## Baseline models accuracy
"""

import xgboost as xgb
models = GetBasedModel()
names,results = BasedLine2(X_train, y_train, models)

"""## Model building

### Random Forest Classifier (criterion = 'entropy')
"""

rf_ent = RandomForestClassifier(criterion='entropy',n_estimators=100)
rf_ent.fit(X_train, y_train)
y_pred_rfe = rf_ent.predict(X_test)

"""### Multi Layer Perceptron"""

mlp = MLPClassifier()
mlp.fit(X_train,y_train)
y_pred_mlp = mlp.predict(X_test)

"""### K nearest neighbour (n=9)"""

knn = KNeighborsClassifier(9)
knn.fit(X_train,y_train)
y_pred_knn = knn.predict(X_test)

"""### Extra Tree Classifier (n_estimators=100)"""

et_500 = ExtraTreesClassifier(n_estimators= 100)
et_500.fit(X_train,y_train)
y_pred_et500 = et_500.predict(X_test)

"""### XGBoost (n_estimators=2000)"""

xgb = xgb.XGBClassifier(n_estimators= 2000)
xgb.fit(X_train,y_train)
y_pred_xgb = xgb.predict(X_test)

"""### Support Vector Classifier (kernel='linear')"""

svc = SVC(kernel='linear',gamma='auto',probability=True)
svc.fit(X_train,y_train)
y_pred_svc = svc.predict(X_test)

"""### Stochastic Gradient Descent"""

sgd = SGDClassifier(max_iter=1000, tol=1e-4)
sgd.fit(X_train,y_train)
y_pred_sgd = sgd.predict(X_test)

"""### Adaboost Classifier"""

ada = AdaBoostClassifier()
ada.fit(X_train,y_train)
y_pred_ada = ada.predict(X_test)

"""### Decision Tree Classifier (CART)"""

decc = DecisionTreeClassifier()
decc.fit(X_train,y_train)
y_pred_decc = decc.predict(X_test)

"""### Light Gradient Boosting Machine classifier"""

lgbmc = LGBMClassifier()
lgbmc.fit(X_train,y_train)
y_pred_lgbmc = lgbmc.predict(X_test)

"""### Gradient boosting machine"""

gbm = GradientBoostingClassifier(n_estimators=100,max_features='sqrt')
gbm.fit(X_train,y_train)
y_pred_gbm = gbm.predict(X_test)

"""## Model"""

import xgboost as xgboost
# selecting list of top performing models to be used in stacked ensemble method
models = [
    #RandomForestClassifier(criterion='entropy',n_estimators=100),
    RandomForestClassifier(criterion='gini',n_estimators=100),
    #ExtraTreesClassifier(n_estimators= 1000),
    ExtraTreesClassifier(n_estimators= 500),
    #ExtraTreesClassifier(n_estimators= 100),

    #KNeighborsClassifier(9),
    #MLPClassifier(),
    #xgboost.XGBClassifier(),
    #xgboost.XGBClassifier(n_estimators= 100),
    xgboost.XGBClassifier(n_estimators= 500),
    #xgboost.XGBClassifier(n_estimators= 1000),
    #xgboost.XGBClassifier(n_estimators= 2000),

    #SGDClassifier(max_iter=1000, tol=1e-4),
    #SVC(kernel='linear',gamma='auto',probability=True),
    #AdaBoostClassifier(),

    DecisionTreeClassifier(),
    LGBMClassifier(),
    #LinearDiscriminantAnalysis(),
    #GradientBoostingClassifier(n_estimators=100,max_features='sqrt'),

]

S_train, S_test = stacking(models,
                           X_train, y_train, X_test,
                           regression=False,

                           mode='oof_pred_bag',

                           needs_proba=False,

                           save_dir=None,

                           metric=accuracy_score,

                           n_folds=5,

                           stratified=True,

                           shuffle=True,

                           random_state=0,

                           verbose=2)

# initializing generalizer model i.e., __ classifier in our case
model = LGBMClassifier()

model = model.fit(S_train, y_train)
y_pred = model.predict(S_test)
print('Final prediction score: [%.8f]' % accuracy_score(y_test, y_pred))

"""## Model Evaluation"""

CM=confusion_matrix(y_test,y_pred)
seaborn.heatmap(CM, annot=True)

TN = CM[0][0]
FN = CM[1][0]
TP = CM[1][1]
FP = CM[0][1]
specificity = TN/(TN+FP)
loss_log = log_loss(y_test, y_pred)
acc= accuracy_score(y_test, y_pred)
roc=roc_auc_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

mathew = matthews_corrcoef(y_test, y_pred)
model_results =pd.DataFrame([['Stacked Classifier',acc, prec,roc,rec,specificity, f1, loss_log,mathew]],
               columns = ['Model', 'Accuracy','Precision','ROC-AUC', 'Sensitivity','Specificity', 'F1 Score','Log_Loss','mathew_corrcoef'])

model_results

"""## Comparison with other Models"""

data = {'Random Forest': y_pred_rfe,
        'EXtra tree classifier': y_pred_et500,
        'Decision Tree Classifier': y_pred_decc,
        'LGBM Classifier': y_pred_lgbmc,
        'GBM': y_pred_gbm,
        'XGB': y_pred_xgb,


                'KNN': y_pred_knn,
                'MLP': y_pred_mlp,
                'SVC': y_pred_svc,
                'SGD': y_pred_sgd,
                'Adaboost': y_pred_ada }

models = pd.DataFrame(data)

for column in models:
    CM=confusion_matrix(y_test,models[column])

    TN = CM[0][0]
    FN = CM[1][0]
    TP = CM[1][1]
    FP = CM[0][1]
    specificity = TN/(TN+FP)
    loss_log = log_loss(y_test, models[column])
    acc= accuracy_score(y_test, models[column])
    roc=roc_auc_score(y_test, models[column])
    prec = precision_score(y_test, models[column])
    rec = recall_score(y_test, models[column])
    f1 = f1_score(y_test, models[column])

    mathew = matthews_corrcoef(y_test, models[column])
    results =pd.DataFrame([[column,acc, prec,roc,rec,specificity, f1, loss_log,mathew]],
               columns = ['Model', 'Accuracy','Precision','ROC-AUC', 'Sensitivity','Specificity', 'F1 Score','Log_Loss','mathew_corrcoef'])
    model_results = model_results.append(results, ignore_index = True)

model_results

"""### ROC AUC Curve"""

def roc_auc_plot(y_true, y_proba, label=' ', l='-', lw=1.0):
    from sklearn.metrics import roc_curve, roc_auc_score
    fpr, tpr, _ = roc_curve(y_true, y_proba[:,1])
    ax.plot(fpr, tpr, linestyle=l, linewidth=lw,
            label="%s (area=%.3f)"%(label,roc_auc_score(y_true, y_proba[:,1])))

f, ax = plt.subplots(figsize=(12,8))


roc_auc_plot(y_test,model.predict_proba(S_test),label='Stacked Ensemble Classifier ',l='-')
roc_auc_plot(y_test,lgbmc.predict_proba(X_test),label='LGBM Classifier',l='-')

roc_auc_plot(y_test,rf_ent.predict_proba(X_test),label='Random Forest Classifier ',l='-')
roc_auc_plot(y_test,et_500.predict_proba(X_test),label='Extra Tree Classifier ',l='-')
roc_auc_plot(y_test,xgb.predict_proba(X_test),label='XGboost Classifier',l='-')


roc_auc_plot(y_test,decc.predict_proba(X_test),label='Decision Tree Classifier',l='-')

ax.plot([0,1], [0,1], color='k', linewidth=0.5, linestyle='--',
        )
ax.legend(loc="lower right")
ax.set_xlabel('False Positive Rate')
ax.set_ylabel('True Positive Rate')
ax.set_xlim([0, 1])
ax.set_ylim([0, 1])
ax.set_title('Receiver Operator Characteristic curves')
seaborn.despine()

"""### Precision Recall curve"""

def precision_recall_plot(y_true, y_proba, label=' ', l='-', lw=1.0):
    from sklearn.metrics import precision_recall_curve, average_precision_score
    precision, recall, _ = precision_recall_curve(y_test,
                                                  y_proba[:,1])
    average_precision = average_precision_score(y_test, y_proba[:,1],
                                                     average="micro")
    ax.plot(recall, precision, label='%s (average=%.3f)'%(label,average_precision),
            linestyle=l, linewidth=lw)

f, ax = plt.subplots(figsize=(14,10))



precision_recall_plot(y_test,model.predict_proba(S_test),label=' Stacking Ensemble classifier ',l='-')
precision_recall_plot(y_test,rf_ent.predict_proba(X_test),label='Random Forest Classifier ',l='-')

precision_recall_plot(y_test,et_500.predict_proba(X_test),label='Extra Tree Classifier ',l='-')
precision_recall_plot(y_test,xgb.predict_proba(X_test),label='XGboost Classifier',l='-')

precision_recall_plot(y_test,lgbmc.predict_proba(X_test),label='LGBM Classifier',l='-')
precision_recall_plot(y_test,decc.predict_proba(X_test),label='Decision Classifier',l='-')

ax.set_xlabel('Recall')
ax.set_ylabel('Precision')
ax.legend(loc="lower left")
ax.grid(True)
ax.set_xlim([0, 1])
ax.set_ylim([0, 1])
ax.set_title('Precision-recall curves')
seaborn.despine()